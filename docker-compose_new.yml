version: '2.1'

services:
  spark-master:
    image: bde2020/spark-master:2.4.0-hadoop2.7
    container_name: spark-master
    hostname: spark-master
    # Attach to the existing Docker network used by your Hadoop cluster
    networks:
      - docker-hadoop-master_default
    ports:
      - "7077:7077"   # Spark master RPC port
      - "8080:8080"   # Spark master web UI
    environment:
      # If you want Spark to talk to HDFS on an existing NameNode
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020

      # Spark in standalone mode
      - SPARK_MODE=master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080

      # Optional environment variable for naming your Spark cluster
      - SPARK_MASTER_NAME=MySparkCluster

    restart: unless-stopped

  spark-worker:
    image: bde2020/spark-worker:2.4.0-hadoop2.7
    container_name: spark-worker
    hostname: spark-worker
    networks:
      - docker-hadoop-master_default
    ports:
      - "8081:8081"   # Spark worker web UI
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - SPARK_MODE=worker
      # Point the worker to the master container
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_WEBUI_PORT=8081

      # Optional environment variables for CPU/Memory constraints in Spark
      # - SPARK_WORKER_CORES=2
      # - SPARK_WORKER_MEMORY=2g

    depends_on:
      - spark-master
    restart: unless-stopped

networks:
  # This references an external network that must already exist 
  # (e.g., from your main docker-compose.yml).
  docker-hadoop-master_default:
    external: true
